<h1 align="center">✨ Awesome PEFT</em> ✨</h1>
<h2 align="center"> PEFT State of the Art Parameter Efficient Fine Tuning Techniques</em> </h2>

<p align="center">

  <img src="https://img.shields.io/github/stars/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=orange&style=for-the-badge" alt="GitHub stars">
  <img src="https://img.shields.io/github/forks/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=blueviolet&style=for-the-badge" alt="GitHub forks">
  <img src="https://img.shields.io/github/last-commit/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=green&style=for-the-badge" alt="GitHub last commit">
  <img src="https://img.shields.io/github/issues/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=crimson&style=for-the-badge" alt="GitHub issues">
  
  <a href="https://GitHub.com/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques/graphs/commit-activity">
    <img src="https://img.shields.io/badge/Maintained%3F-yes-brightgreen.svg?style=for-the-badge" alt="Maintenance">
  </a>

</p>

<p align="center">
    <img src="https://i.imgur.com/waxVImv.png" alt="Oryx Video-ChatGPT">
</p>

## 🚀 Serial Adapters

- Parameter-Efficient Transfer Learning for NLP [[Paper](https://arxiv.org/abs/1902.00751)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/google-research/adapter-bert)

- AdapterHub: A Framework for Adapting Transformers [[Paper](https://arxiv.org/abs/2007.07779)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adapterhub-a-framework-for-adapting)

- MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer [[Paper](https://arxiv.org/abs/2005.00052)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/mad-x-an-adapter-based-framework-for-multi)

- Cross-Lingual Transfer with Target Language-Ready Task Adapters [[Paper](https://aclanthology.org/2023.findings-acl.13.pdf)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/parovicm/tlr-adapters?tab=readme-ov-file)

- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning [[Paper](https://arxiv.org/pdf/1902.02671)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AsaCooperStickland/Bert-n-Pals)



## 🚀 Parallel Adapters

- UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling [[Paper](https://arxiv.org/pdf/2302.06605)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/UniAdapter/UniAdapter)

- UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory [[Paper](https://arxiv.org/pdf/2308.14316)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Paranioar/UniPT)

- AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition [[Paper](https://arxiv.org/pdf/2205.13535)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ShoufaChen/AdaptFormer)

- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning [[Paper](https://arxiv.org/pdf/1902.02671)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AsaCooperStickland/Bert-n-Pals?tab=readme-ov-file)

- PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning [[Paper](https://arxiv.org/pdf/2402.15082)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/JachinLin2022/PEMT)

- Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference [[Paper](https://arxiv.org/pdf/2304.04947)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/conditional-adapters-parameter-efficient)


## 🚀 Hybrid Adapters

- AUTOPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2301.12132)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/autopeft)

- CROSS-MODAL ADAPTER: PARAMETER-EFFICIENT TRANSFER LEARNING APPROACH FOR VISION-LANGUAGE MODELS [[Paper](https://arxiv.org/pdf/2404.12588)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/cross-modal-adapter-parameter-efficient)

- EFFICIENT REMOTE SENSING WITH HARMONIZED TRANSFER LEARNING AND MODALITY ALIGNMENT [[Paper](https://arxiv.org/pdf/2404.18253)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/seekerhuang/HarMA?tab=readme-ov-file)

- MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval [[Paper](https://arxiv.org/pdf/2301.07868)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/zhangbw17/MV-Adapter)

- Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets [[Paper](https://arxiv.org/pdf/2208.07463)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Hhhhhhao/Conv-Adapter)

- Conditional Adapters: Parameter-Efficient Transfer Learning with Fast Inference [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/19d7204af519eae9993f7f72377a0ec0-Paper-Conference.pdf)]
  ![NeurIPS](https://img.shields.io/badge/NeurIPS-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/conditional-adapters-parameter-efficient)


## 🚀 Single Task

- VISION TRANSFORMER ADAPTER FOR DENSE PREDICTIONS [[Paper](https://arxiv.org/pdf/2205.08534)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/czczup/ViT-Adapter)

- Simple, Scalable Adaptation for Neural Machine Translation [[Paper](https://aclanthology.org/D19-1165.pdf)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/simple-scalable-adaptation-for-neural-machine)

- K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters [[Paper](https://arxiv.org/pdf/2002.01808)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/K-Adapter)


## 🚀 Multi Task

- K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters [[Paper](https://arxiv.org/pdf/2002.01808)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/K-Adapter)

- AdapterFusion: Non-Destructive Task Composition for Transfer Learning [[Paper](https://arxiv.org/pdf/2005.00247)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adapterfusion-non-destructive-task)

- OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy [[Paper](https://arxiv.org/pdf/2401.10559)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Multi-Head Adapter Routing for Cross-Task Generalization [[Paper](https://arxiv.org/pdf/2211.03831)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/mttl)

- Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks [[Paper](https://arxiv.org/pdf/2106.04489)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/rabeehk/hyperformer)

- When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.319.pdf)]
  ![arXiv](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AGI-Edgerunners/LLM-Adapters)

- AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models [[Paper](https://arxiv.org/pdf/2302.07027)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adaptersoup-weight-averaging-to-improve)


## 🚀 Continuous Prompting 

- Prefix-Tuning: Optimizing Continuous Prompts for Generation [[Paper](https://arxiv.org/pdf/2101.00190)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/XiangLi1999/PrefixTuning)

- PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent Representation MOdification [[Paper](https://www.arxiv.org/pdf/2409.17834)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- DEPT: Decomposed Prompt Tuning for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2309.05173)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ZhengxiangShi/DePT)

- P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks [[Paper](https://arxiv.org/pdf/2110.07602)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/THUDM/P-tuning-v2)

- Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models [[Paper](https://arxiv.org/pdf/2404.04522)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- PTR: Prompt Tuning with Rules for Text Classification [[Paper](https://arxiv.org/pdf/2105.11259)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/thunlp/PTR)

- Prefix-Propagation: Parameter-Efficient Tuning for Long Sequences [[Paper](https://web3.arxiv.org/pdf/2305.12086)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MonliH/prefix-propagation)

- Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts [[Paper](https://arxiv.org/pdf/2210.11292)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/xyltt/LPT)

- OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [[Paper](https://arxiv.org/pdf/2402.04129)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jpmorganchase/ovor)


## 🚀 Discrete Prompt

- RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning [[Paper](https://arxiv.org/pdf/2205.12548)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mingkaid/rl-prompt)

- SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations [[Paper](https://arxiv.org/pdf/2305.13235)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AkihikoWatanabe/paper_notes/issues/1684)

- OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [[Paper](https://arxiv.org/pdf/2402.04129)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jpmorganchase/ovor)

  


## 🚀 Domain Specific Adaption (Natural Language Understanding)

- Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation [[Paper](https://arxiv.org/pdf/2405.15282)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jabhinav/Prompt-Tuning-Strikes-Back-with-LOPA)

- InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding [[Paper](https://arxiv.org/pdf/2306.04933)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/infoprompt-information-theoretic-soft-prompt)

- PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization [[Paper](https://arxiv.org/pdf/2407.18078)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ChrisIsKing/Parameter-Efficient-Personalization)

- IDPG: An Instance-Dependent Prompt Generation Method [[Paper](https://arxiv.org/pdf/2204.04497)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/CSerxy/IDPG)

- APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.567.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wgcban/apt)

- SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts [[Paper](https://aclanthology.org/2023.emnlp-main.884.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jyjohnchoi/SMoP)



## 🚀 Task Specific Adaption

- The Power of Scale for Parameter-Efficient Prompt Tuning [[Paper](https://arxiv.org/pdf/2104.08691)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mkshing/Prompt-Tuning)

- SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer [[Paper](https://arxiv.org/pdf/2110.07904)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/spot-better-frozen-model-adaptation-through)

- APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference [[Paper](https://arxiv.org/html/2401.12200v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- XPROMPT: Exploring the Extreme of Prompt Tuning [[Paper](https://arxiv.org/pdf/2210.04457)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/BD-MF/XPrompt?tab=readme-ov-file)

- Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts [[Paper](https://aclanthology.org/2023.findings-emnlp.584.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mlwu22/TPT)

- IDPG: An Instance-Dependent Prompt Generation Method [[Paper](https://arxiv.org/pdf/2204.04497)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/CSerxy/IDPG)

- APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.567.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wgcban/apt)

- SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts [[Paper](https://aclanthology.org/2023.emnlp-main.884.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jyjohnchoi/SMoP)




## 🚀 Scaling Adaption

- Propulsion: Steering LLM with Tiny Fine-Tuning [[Paper](https://arxiv.org/pdf/2409.10927)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Kowsher/Propulsion)


## 🚀 Selective Tuning Based on Parameter Importance

- Parameter-Efficient Transfer Learning with Diff Pruning [[Paper](https://arxiv.org/pdf/2012.07463)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/dguo98/DiffPruning)

- Targeted Efficient Fine-tuning: Optimizing Parameter Updates with Data-Driven Sample Selection [[Paper](https://arxiv.org/pdf/2403.08484)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models [[Paper](https://arxiv.org/pdf/2410.11772)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Kaiseem/IST)

- AdaFish: Fast Low-Rank Parameter-Efficient Fine-Tuning by Using Second-Order Information [[Paper](https://arxiv.org/pdf/2403.13128v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adafish-fast-low-rank-parameter-efficient)

- Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning [[Paper](https://arxiv.org/pdf/2311.03748)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/psunlpgroup/FISH-DIP)



## 🚀 Unstructured Mask

- Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models [[Paper](https://arxiv.org/pdf/2305.16597)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/neural-architecture-search-for-parameter)

- Composable Sparse Fine-Tuning for Cross-Lingual Transfer [[Paper](https://arxiv.org/pdf/2110.07560)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/composable-sft)

- Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning [[Paper](https://arxiv.org/pdf/2109.05687)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/RunxinXu/ChildTuning)

- Parameter-Efficient Fine-Tuning without Introducing New Latency [[Paper](https://arxiv.org/pdf/2305.16742)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/parameter-efficient-fine-tuning-without)



## 🚀 Structured Mask


- Efficient Fine-Tuning of BERT Models on the Edge [[Paper](https://arxiv.org/pdf/2205.01541)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/efficient-fine-tuning-of-bert-models-on-the)

- Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation [[Paper](https://arxiv.org/pdf/2104.08771)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MGheini/xattn-transfer-for-mt)

- X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme Multi-Profile Scenarios [[Paper](https://arxiv.org/pdf/2401.16137)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning [[Paper](https://ar5iv.labs.arxiv.org/html/2406.17740)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/arijitthegame/structured-matrices-PEFT)


## 🚀 Core Low Rank Decomposition


- LoRA: Low-Rank Adaptation of Large Language Models [[Paper](https://arxiv.org/pdf/2106.09685)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/LoRA)

- Compacter: Efficient Low-Rank Hypercomplex Adapter Layers [[Paper](https://arxiv.org/pdf/2106.04647)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/rabeehk/compacter)

- Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning [[Paper](https://arxiv.org/pdf/2012.13255)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/intrinsic-dimensionality-explains-the)

- Parameter-Efficient Model Adaptation for Vision Transformers [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/25160)]
  ![AAAI](https://img.shields.io/badge/AAAI-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)

- Parameter-Efficient Fine-Tuning without Introducing New Latency [[Paper](https://arxiv.org/pdf/2305.16742)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/parameter-efficient-fine-tuning-without)

- DoRA: Weight-Decomposed Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2402.09353)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/NVlabs/DoRA)

- LLMEmbed: Rethinking Lightweight LLM’s Genuine Function in Text Classification [[Paper](https://arxiv.org/pdf/2406.03725)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ChunLiu-cs/LLMEmbed-ACL2024)



## 🚀 Adaptive and dynamic rank methods

- DyLoRA: Parameter-Efficient Tuning of Pretrained Models using Dynamic Search-Free Low Rank Adaptation [[Paper](https://arxiv.org/pdf/2210.07558)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/dylora-parameter-efficient-tuning-of-pre)

- AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2303.10512)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/QingruZhang/AdaLoRA)

- Sparse Low-Rank Adaptation of Pre-trained Language Models [[Paper](https://arxiv.org/pdf/2311.11696)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TsinghuaC3I/SoRA)

- Increasing Model Capacity for Free: A Simple Strategy for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2407.01320)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/LINs-lab/CapaBoost)

- AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning [[Paper](https://arxiv.org/pdf/2403.09113)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://anonymous.4open.science/r/AutoLoRA)



## 🚀 Enhanced LoRA variants for fine tuning efficiency

- Bayesian Low-Rank Adaptation for Large Language Models [[Paper](https://arxiv.org/html/2308.13111v5)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MaximeRobeyns/bayesian_lora)

- LoRA Dropout as a Sparsity Regularizer for Overfitting Control [[Paper](https://arxiv.org/html/2404.09610v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/lora-dropout-as-a-sparsity-regularizer-for)

- PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization [[Paper](https://arxiv.org/pdf/2402.16141)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/periodiclora-breaking-the-low-rank-bottleneck)

- LoRA+: Efficient Low Rank Adaptation of Large Models [[Paper](https://arxiv.org/pdf/2402.12354)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/LoRA)

- Mixture-of-Subspaces in Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.11909)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wutaiqiang/MoSLoRA)

- Continual Learning with Low Rank Adaptation [[Paper](https://arxiv.org/pdf/2311.17601)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/continual-learning-with-low-rank-adaptation)

- Trans-LoRA: Towards Data-Free Transferable Parameter Efficient Finetuning [[Paper](https://arxiv.org/html/2405.17258v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/textit-trans-lora-towards-data-free)

- RoseLoRA: Row and Column-wise Sparse Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.10777)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/lliutianc/roselora)

- Low-Rank Few-Shot Adaptation of Vision-Language Models [[Paper](https://arxiv.org/html/2405.18541v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MaxZanella/CLIP-LoRA)

- SVDQUANT: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models [[Paper](https://arxiv.org/pdf/2411.05007)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file)

- Variational Low-Rank Adaptation Using IVON [[Paper](https://arxiv.org/pdf/2411.04421)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/team-approx-bayes/ivon-lora)

- PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment [[Paper](https://arxiv.org/pdf/2411.01245)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/pmol-parameter-efficient-moe-for-preference)

- Empower Vision Applications with LoRA LMM [[Paper](https://arxiv.org/pdf/2411.00915)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition [[Paper](https://arxiv.org/pdf/2307.13269)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/sail-sg/lorahub)

- MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning [[Paper](https://arxiv.org/pdf/2309.05444)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/for-ai/parameter-efficient-moe)

- Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models [[Paper](https://arxiv.org/html/2403.03432v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/mixture-of-loras-an-efficient-multitask)

- MIXTURE OF LORA EXPERTS [[Paper](https://arxiv.org/pdf/2404.13628)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts [[Paper](https://arxiv.org/html/2404.15159v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TUDB-Labs/MixLoRA)



## 🚀 Hybrid Approaches 


- Towards a Unified View of Parameter-Efficient Transfer Learning [[Paper](https://arxiv.org/pdf/2110.04366)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jxhe/unify-parameter-efficient-tuning?tab=readme-ov-file)

- UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning [[Paper](https://arxiv.org/pdf/2110.07577)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/morningmoni/UniPELT)

- Parameter-Efficient Fine-Tuning Design Spaces [[Paper](https://arxiv.org/pdf/2301.01821)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/amazon-science/peft-design-spaces)

- Neural Prompt Search [[Paper](https://arxiv.org/pdf/2206.04673)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ZhangYuanhan-AI/NOAH)

- AUTOPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2301.12132)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/autopeft)

- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models [[Paper](https://arxiv.org/pdf/2304.01933)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AGI-Edgerunners/LLM-Adapters)

- RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation [[Paper](https://arxiv.org/pdf/2401.04679)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/IST-DASLab/RoSA)

- Sparsity- and Hybridity-Inspired Visual Parameter-Efficient Fine-Tuning for Medical Diagnosis [[Paper](https://arxiv.org/pdf/2405.17877)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks [[Paper](https://arxiv.org/pdf/2203.03878)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/hyperpelt-unified-parameter-efficient)

- Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning [[Paper](https://arxiv.org/pdf/2309.06922)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/extremebird/Hydra)


## 🚀 MoE Based

- When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts [[Paper](https://arxiv.org/pdf/2404.15159)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TUDB-Labs/MixLoRA)

- Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning [[Paper](https://arxiv.org/pdf/2309.05444)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/for-ai/parameter-efficient-moe)

- Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models [[Paper](https://arxiv.org/pdf/2403.03432)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://gist.github.com/ruvnet/809d0312c1c599ba29721c93a20a741c)

- Mixture-of-Subspaces in Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.11909)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/👩‍💻%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wutaiqiang/MoSLoRA)



# PEFT Methods in NLP Tasks

This repository contains a collection of Parameter-Efficient Fine-Tuning (PEFT) methods for Natural Language Processing tasks.

## Models and Papers

| Year | Model | Paper Title | Links |
|---------|-------|-------------|-------|
| 31 July 2024 | LLaMA3-8B | The Llama 3 Herd of Models | [arXiv](https://arxiv.org/abs/2407.21783) [GitHub](https://github.com/meta-llama/llama3) |
| 4 Jan 2024 | TinyLlama | TinyLlama: An Open-Source Small Language Model | [arXiv](https://arxiv.org/abs/2401.02385) [GitHub](https://github.com/jzhang38/TinyLlama) |
| 21 Nov 2023 | ShareGPTv4(7B) | ShareGPT4V: Improving Large Multi-Modal Models with Better Captions | [arXiv](https://arxiv.org/abs/2311.12793) [GitHub](https://sharegpt4v.github.io/) |
| 24 Aug 2023 | Qwen-VL-Chat(7B) | Qwen-VL: A Vision-Language Foundation Model for Universal Multimodal Understanding and Generation | [arXiv](https://arxiv.org/abs/2308.12966) [GitHub](https://github.com/QwenLM/Qwen-VL) |
| 15 May 2024 | LLaVA-1.5 | Improved Baselines with Visual Instruction Tuning | [arXiv](https://arxiv.org/abs/2310.03744) [GitHub](https://github.com/haotian-liu/LLaVA) |
| 10 Oct 2023 | Mistral-7B-Instruct | Mistral 7B | [arXiv](https://arxiv.org/abs/2310.06825) [GitHub](https://github.com/mistralai/mistral-inference) |
| 16 Mar 2024 | Openchat8B | OpenChat: Advancing Open-source Language Models with Mixed-Quality Data | [arXiv](https://arxiv.org/abs/2309.11235) [GitHub](https://github.com/imoneoi/openchat) |
| 18 June 2024 | ChatGLM-6B | ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools | [arXiv](https://arxiv.org/abs/2406.12793) [GitHub](https://github.com/THUDM/ChatGLM-6B) |
| 14 Feb 2024 | LlaSMol | LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset | [arXiv](https://arxiv.org/abs/2402.09391) [GitHub](https://github.com/OSU-NLP-Group/LLM4Chem) |
| 26 June 2024 | Tag-LLaMA | Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains | [arXiv](https://arxiv.org/abs/2402.05140) |
| 31 May 2024 | Mamba-2 | Mamba: Linear-Time Sequence Modeling with Selective State Spaces | [arXiv](https://arxiv.org/abs/2405.21060) [GitHub](https://github.com/state-spaces/mamba) |
| Dec 1 2023 | Mamba | Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality | [arXiv](https://arxiv.org/abs/2312.00752) [GitHub](https://github.com/state-spaces/mamba) |
| 14 Dec 2024 | TigerBot-7B | TigerBot: A Multi-stage Open-Source LLM for Human and Agent Scenarios | [arXiv](https://arxiv.org/abs/2312.08688) [GitHub](https://github.com/TigerResearch/TigerBot) |
| 16 April 2024 | GEMMA-2B,  Gemma-7B | Gemma: Open Models Based on Gemini Research and Technology | [arXiv](https://arxiv.org/abs/2403.08295) [GitHub](https://github.com/google-gemma/gemma) |
| 2024 | DeepSeek-Coder-Base-6.7B | DeepSeek Coder: When the Large Language Model Meets Programming | [arXiv](https://arxiv.org/abs/2401.14196) [GitHub](https://github.com/deepseek-ai/DeepSeek-Coder) |
| 7 May 2021 | CURE | Code-Aware Neural Machine Translation for Automatic Program Repair | [Paper](https://ieeexplore.ieee.org/abstract/document/9401997) |
| 10 April 2022 | RewardRepair | Neural Program Repair with Execution-based Backpropagation | [arXiv](https://arxiv.org/abs/2105.04123) |
| 18 Aug 2024 | Recorder | A Syntax-Guided Edit Decoder for Neural Program Repair | [Paper](https://dl.acm.org/doi/10.1145/3468264.3468544) |
| 24 Aug 2023 | CodeLlama-7B | Code Llama: Open Foundation Models for Code | [arXiv](https://arxiv.org/abs/2308.12950) [GitHub](https://github.com/facebookresearch/codellama) |
| 18 July 2023 | Llama2-7b,3b,13b,70b | Llama 2: Open Foundation and Fine-Tuned Chat Models | [arXiv](https://arxiv.org/abs/2307.09288) [GitHub](https://github.com/facebookresearch/llama) |
|27 Feb 2023 | LLaMA30B | LLaMA: Open and Efficient Foundation Language Models | [arXiv](https://arxiv.org/abs/2302.13971) [GitHub](https://github.com/facebookresearch/llama) |
| 12 April 2022 | INCODER-1B,6B | InCoder: A Generative Model for Code Infilling and Synthesis | [arXiv](https://arxiv.org/abs/2204.05999) [GitHub](https://github.com/dpfried/incoder) |
| 9 June 2023 | Vicuna-7b,13b | Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena | [arXiv](https://arxiv.org/abs/2306.05685) [GitHub](https://github.com/lm-sys/FastChat) |
| 19 April 2023 | Baichuan2-13B | Baichuan 2: Open Large-scale Language Models | [arXiv](https://arxiv.org/abs/2309.10305) [GitHub](https://github.com/baichuan-inc/Baichuan2) |
|1 Nov 2019 | DialoGPT | DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation | [arXiv](https://arxiv.org/abs/1911.00536) [GitHub](https://github.com/microsoft/DialoGPT) |
| 11 Oct 2018 | BERT-Base,Large | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [arXiv](https://arxiv.org/abs/1810.04805) [GitHub](https://github.com/google-research/bert) |
| 18 Nov 2021 | DeBERTaV3-base | DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing | [arXiv](https://arxiv.org/abs/2111.09543) [GitHub](https://github.com/microsoft/DeBERTa) |
| 30 Nov 2023 | CCT5 | A Code-Change-Oriented Pre-trained Model | [Paper](https://dl.acm.org/doi/abs/10.1145/3611643.3616339) |
| 2 Sep 2021 | CodeT5 | CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation | [arXiv](https://arxiv.org/abs/2109.00859) [GitHub](https://github.com/salesforce/CodeT5) |
| 25 Mar 2022 | CodeGen | CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis | [arXiv](https://arxiv.org/abs/2203.13474) [GitHub](https://github.com/salesforce/CodeGen) |
| 16 Nov 2022 | Galactica | Galactica: A Large Language Model for Science | [arXiv](https://arxiv.org/abs/2211.09085) [GitHub](https://github.com/paperswithcode/galai) |
| 29 Jan 2023 | Text+Chem T5 | Unifying Molecular and Textual Representations via Multi-task Language Modelling | [arXiv](https://arxiv.org/abs/2301.12586) |
| 27 June 2023 | BLOOM-7B,1B | BLOOM: A 176B-Parameter Open-Access Multilingual Language Model | [arXiv](https://arxiv.org/abs/2211.05100) [GitHub](https://github.com/bigscience-workshop/multilingual-modeling) |
| 29 Nov 2023 | Falcon |The Falcon Series of Open Language Models | [arXiv](https://arxiv.org/pdf/2311.16867)|
| 15 Mar 2023 | GPT-4 | GPT-4 Technical Report | [arXiv](https://arxiv.org/abs/2303.08774) |
| 2024 | GPT-J (6B) | GPT-J: 6B Parameter Open Source Transformer Model | [GitHub](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GPT-J-6B) |
| Aug 2021 | GPT-Neo | GPT-Neo: Implementation of Model Parallel GPT2-like Models | [GitHub](https://github.com/EleutherAI/gpt-neo) |
| Feb 2019 | GPT2 (GPT2, GPT2-M, GPT2-L) | Language Models are Unsupervised Multitask Learners | [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [GitHub](https://github.com/openai/gpt-2) |
| Nov 30 2022 | GPT-3.5 | GPT-3.5 Technical Report | [Blog](https://openai.com/blog/chatgpt) |
| 2 May 2022 | OPT (OPT, OPT-13B, OPT-6.7B, OPT-1.3B) | OPT: Open Pre-trained Transformer Language Models | [arXiv](https://arxiv.org/abs/2205.01068) [GitHub](https://github.com/facebookresearch/metaseq) |
| 26 July 2019 | RoBERTa (RoBERTa-Large, RoBERTa-Base) | RoBERTa: A Robustly Optimized BERT Pretraining Approach | [arXiv](https://arxiv.org/abs/1907.11692) [GitHub](https://github.com/pytorch/fairseq/tree/main/examples/roberta) |
| 23 Oct 2019 | T5 (T5-BASE, T5-SMALL, T5-Large) | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | [arXiv](https://arxiv.org/abs/1910.10683) [GitHub](https://github.com/google-research/text-to-text-transfer-transformer) |
| 20 Oct 2022 | Flan-T5 (Flan-T5-base, Flan-T5-xl) | Scaling Instruction-Finetuned Language Models | [arXiv](https://arxiv.org/abs/2210.11416) [GitHub](https://github.com/google-research/t5x) |
| 29 Oct 2019 | BART-Large | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | [arXiv](https://arxiv.org/abs/1910.13461) [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/bart) |
| 22 Oct 2020 | ViT | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [arXiv](https://arxiv.org/abs/2010.11929) [GitHub](https://github.com/google-research/vision_transformer) |
| 26 Sep 2019 | ALBERT | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | [arXiv](https://arxiv.org/abs/1909.11942) [GitHub](https://github.com/google-research/albert) |
| 2 Oct 2019 | DistilBERT | DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter | [arXiv](https://arxiv.org/abs/1910.01108) [GitHub](https://github.com/huggingface/transformers/tree/main/src/transformers/models/distilbert) |
| 28 May 2020 | GPT-3 | Language Models are Few-Shot Learners | [arXiv](https://arxiv.org/abs/2005.14165) |
| 14 Feb 2020 | TwinBERT | TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval | [arXiv](https://arxiv.org/abs/2002.06275) |
| 27 April 2020 | ColBERT | ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT | [arXiv](https://arxiv.org/abs/2004.12832) [GitHub](https://github.com/stanford-futuredata/ColBERT) |
| 8 Nov 2019 | mBART | How Language-Neutral is Multilingual BERT? | [arXiv](https://arxiv.org/abs/1911.03310) [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/mbart) |
| 23 Mar 2020 | ELECTRA | ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | [arXiv](https://arxiv.org/abs/2003.10555) [GitHub](https://github.com/google-research/electra) |
|5 Oct 2022 | PaLM | PaLM: Scaling Language Modeling with Pathways | [arXiv](https://arxiv.org/abs/2204.02311) |
| July 2002 | BLEU | BLEU: A Method for Automatic Evaluation of Machine Translation | [Paper](https://aclanthology.org/P02-1040.pdf) |
| 29 Apr 2021 | EFL | Entailment as Few-Shot Learner | [arXiv](https://arxiv.org/abs/2104.14690) |
| 5 June 2020 | DeBERTaLARGE,DeBERTa | DeBERTa: Decoding-enhanced BERT with Disentangled Attention | [arXiv](https://arxiv.org/abs/2006.03654) [GitHub](https://github.com/microsoft/DeBERTa) |
| 5 Aug 2022 | BlenderBot | BlenderBot 3: A Deployed Conversational Agent That Continually Learns to Responsibly Engage | [arXiv](https://arxiv.org/abs/2208.03188) |
| Sep 2022 | GIZA++ | Embedding-Enhanced GIZA++: Improving Word Alignment Using Embeddings | [Paper](https://aclanthology.org/2022.amta-research.20.pdf) |
| 5 Nov 2019 | XLM-R (xlm-roberta-base) | Unsupervised Cross-lingual Representation Learning at Scale | [arXiv](https://arxiv.org/abs/1911.02116) [GitHub](https://github.com/pytorch/fairseq/tree/main/examples/xlmr) |
| 18 Apr 2020 | SimAlign | SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings | [arXiv](https://arxiv.org/abs/2004.08728) [GitHub](https://github.com/cisnlp/simalign) |
| 4 Mar 2020 | jiant | jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models | [arXiv](https://arxiv.org/abs/2003.02249) [GitHub](https://github.com/nyu-mll/jiant) |
| 6 Apr 2020 | MobileBERT | MobileBERT: A Compact Task-Agnostic BERT for Resource-Limited Devices | [arXiv](https://arxiv.org/abs/2004.02984) |
| 12 June 2017 | Transformer | Attention Is All You Need | [arXiv](https://arxiv.org/abs/1706.03762) [GitHub](https://github.com/tensorflow/tensor2tensor) |
| 10 Apr 2020 | Longformer | Longformer: The Long-Document Transformer | [arXiv](https://arxiv.org/abs/2004.05150) [GitHub](https://github.com/allenai/longformer) |
| 19 June 2019 | XLNet | XLNet: Generalized Autoregressive Pretraining for Language Understanding | [arXiv](https://arxiv.org/abs/1906.08237) [GitHub](https://github.com/zihangdai/xlnet) |
| 26 Feb 2024 | Airavata-7b | Airavata: Introducing Hindi Instruction-tuned LLM | [arXiv](https://arxiv.org/html/2401.15006v2) |








  
